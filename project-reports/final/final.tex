%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=10pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=0.6in]{geometry} % this controls page margin
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
%\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
%\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

%\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{Stanford CS 229 Fall 2017} \\ [20pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\Large Final Project Report: \\
\Large Real Time Tennis Match Prediction Using Machine Learning\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Yang "Eddie" Chen, Yubo Tian, Yi Zhong} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}
\twocolumn[
\maketitle % Print the title
]
%----------------------------------------------------------------------------------------
%	Section 1 Motivation
%----------------------------------------------------------------------------------------
\begin{abstract}
This project adopts an innovative data model by combining both historical match data and real-time stats, and apply machine learning to predict tennis match outcomes. Specifically, we explore and compare four data models mixing historical data and real-time stats, while applying machine learning techniques such as logistic regression, support vector classification (SVC) with linear, RBF and polynomial kernels, and Naive Bayes classification. Feature selection techniques such as recursive feature elimination and principal component analysis were employed as well. We find that applying SVC with a RBF kernel on historical data alone gives the best prediction. More importantly, a thorough analysis on results was done to reveal and understand the predominant challenge in this hybrid approach - high bias, as our extracted feature set does not cover edge cases and "comback" situations. \textbf{TTL} (total points won) is found to be the most dominant and predictive feature for linear models, with other standout features also revealed. From there, the project outlines the future work needed to combat the high bias issue by proposing other features that could be used but not included in the current project. 
\end{abstract}

\section{Introduction}
In the 2017 Stuttgart Open, 18-time grand slam champion Roger Federer lost to the world No. 302 player Tommy Haas on a grass court, which hadn't happened since 2002. Based on past performance alone, almost any model would have predicted a Federer win, perhaps with large margin too. This surprise event motivated us to apply machine learning to predict tennis matches in real time; in particular, we want to apply a hybrid model that combines historical and real-time, in-game information, in an attempt to better existing predictions. 
%----------------------------------------------------------------------------------------
%	Section - Method
%----------------------------------------------------------------------------------------

\section{Related Work}
Extensive research has been done on tennis as it is an ideal sport to apply hierarchical probability models: tennis match consists of a sequence of sets, which in turn consist of several games, which in turn consist of a sequence of points. Knottenbelt \cite{KNOTTENBELT20123820} proposed a common opponent model to extract features between two players that is found to be very predcitve in his stochastic models, while James \cite{omalley} has done thorough study on probability formulas and statistical analysis on tennis matches. Recent researchers began to apply machine learning on tennis pre-game predictions based on past performance (including previous encounter, current ranking, etc), such as Sipko \cite{tennis1}, and Madurska\cite{tennis2setbyset} - whose by-set method inspired us. 

Our project seeks to apply classification algorithms from machine learning to model men's professional singles matches by using both pre-game, historical performance calcualted via a common opponent model \cite{KNOTTENBELT20123820}, and real-time, in-game stats \cite{tennis_charting} \cite{tennis2setbyset}. As pointed out by Sipko \cite{tennis1}, this in-game approach "allows the model to capture the change in a player's performance over the course of the match. For example, different players fatigue during a match in different ways." We hope that results from our predictions can be extended to give real-time coaching advice to support game strategy decision.
%----------------------------------------------------------------------------------------
%	Section - Method
%----------------------------------------------------------------------------------------

\section{Dataset and Features}
\subsection{Dataset \& pre-processing}
This project employs two open source datasets for labels and features, authored by Jeff Sackmann \cite{tennis_atp}\cite{tennis_charting}: the \textit{Match Charting Project}, which contains set level data from 1442 matches spanning from 1974 to 2017; and the \textit{Tennis ATP} dataset which contains match level data from 1969 to 2017. There are a few pre-processing steps needed to make the data useful, such as: 
\begin{itemize}
\item \textbf{Picking date range}: Matches from 2000 - 2017 are used as samples (1415 matches) to focus on the comtemporary game of Tennis.
\item \textbf{Removing duplicates via match id}: Since both datasets are manually entered via crowdsourcing, there are some inaccuracy and duplication that require our pre-processing.
\item \textbf{Removing Davis Cup}: Davis Cup is the World Cup of tennis, where players play for their country.  We decided to remove all Davis Cup entries since group matches may include strategic moves that are considerably different from individual tournaments.
\item \textbf{Merging Tennis ATP \& Match Charting Project}: Real-time stats are only found in the \textit{Match Charting Project} dataset whereas historical performance can be computed from \textit{Tennis ATP}. Two datasets need to be merged to give us necessary flexibility and features to test our ideas. They are merged on player's first and last names, with the rare cases of players with duplicate names removed manually.
\item \textbf{Standardizing Features} We are unable to extract all features listed in Sipko \cite{tennis1}. While extracting features from the \textit{Tennis ATP} was easy, the \textit{Match Charting Project} data proves non-trivial with its detailed stats and fragmented file structure.  We managed to extract a subset of all possible features from processing the point-by-point data, and standardized across the historical and current features.
\end{itemize}
\subsection{Feature Extraction}
\label{sec:label}
The original dataset denotes Player 1 as the winner and Player 2 as the loser. While processing, we decide to represent each set in each match with \textit{two} rows, one from each player's perspective, and label the match result accordingly (1 and -1 for win and loss, respectively). Thus we have one row per player per set per match, containing features from historical data and real time data. Historical data includes match details and player's past performance; real-time data, for the scope of this project, will be defined as the performance from all previous sets in the match:
\begin{enumerate}
\item A vector of input features (\textbf{X}), describing the performance in previous set and historical performance from one player's perspective
\item A target value (\textbf{y}), indicating the match outcome. $y \in \{1,-1\}$
\end{enumerate}
\subsubsection{Symmetric Feature Representation}
There are two approaches to represent features from two players.  First, we can have two features for the same metric (e.g. $RANK_1$ and $RANK_2$).  This approach doubles the number of features, but also provides a quantitative measure that we can compare across all players.  Second, we can look at \textit{differences} between two players (e.g. $RANK_{DIFF} = RANK_1 - RANK_2$).  This would reduce the number of features, but does not allow the model to compare across players.  Previous research has shown that the difference variables are predictive enough \cite{tennis1} \cite{omalley} for past performance (on match level).  For current performance (on set level), we evaluated the performance of both approaches. There was no difference in terms of accuracy, hence the difference model was used for current match as well for consistency.
\subsubsection{Common Opponent Model for Past Performance}
We use a method proposed by Knottenbelt \cite{KNOTTENBELT20123820} and extended by Sipko \cite{tennis1} to construct \textit{difference variables} capturing player characteristics and past performance, by using their \textbf{common opponents} to achieve a fair comparison. The detailed code can be found in our codebase, while the process works as follows: 
\begin{itemize}
\item Identify a set of common opponents between the two players of a given match.
\item Compute the average performance of both players against the common opponent.
\item Take the difference between average performance for each extracted feature.
\end{itemize}
\subsubsection{Edge Cases for Common Opponents}
We have run into 2 edge cases: (1) when at least one of the players is new; (2) when two players do not have any common opponents. Upon investigation, our solution is: (1) when the player is new with no historical data, we remove the samples, which consist of 84 data points out of 1421 matches; (2) when two players do not have any common opponents, we compute the average for each player (regardless of opponents) and take the difference, which consist of 74 out of 1421 matches. Note, as described in Section~\ref{sec:label}, our final sample size will contain $2x * 1421$ rows, where $x$ is the number of sets played in each match.

It's worth noting that whenever we encounter NULLs in the feature dataset, we remove the entire row to avoid the confusion introduced by potentially using different null-filling options, and to be consistent and rigorous. This reduces available data points for training and evaluation, and could potentially impact model performance. 
\subsection{Feature Lists}
\label{sec:feat}
\subsubsection{Features from historical matches}
Features are the \textit{difference variables} averaged for both players obtained from a common opponent model \cite{KNOTTENBELT20123820}, unless otherwise noted. They are:
\begin{itemize}
\item \textbf{Performance Features}: Number of aces, double faults, break points faced and saved; Percentages of winning on first and second serves, and first serve in rate.
\item \textbf{Match Details}: Match duration in minutes, number of serve games, number of serve points per game
\item \textbf{Player Bio}: ATP Rank, ATP Rank Points, same handedness, height
\end{itemize}
We also extracted and added two more features to the models after the first pass of the project and diagnostics that reveal high bias as our main hindrance of performance, which we will elaborate further in Section 5. The two added features are \textbf{duration win/loss rate} which calculates the average difference in duration (in minutes) of the past matches each player won or lost respectively, using the Common Opponent Model, and \textbf{age diff win/loss rate}, which calculates the average age difference of the past matches each player won or lost respectively, using the same Common Opponent Model.
\subsubsection{Features from current match performance}
\begin{itemize}
\item \textbf{Performance Features} Number of aces, double faults, winners and total points won; Percentage of winning on first and second serve, return points won, first serve in
\item \textbf{Match Details} Surface (hard, grass, or clay), match type
\item \textbf{Player Bio} Same handedness
\end{itemize}
%----------------------------------------------------------------------------------------
%	Section - Method
%----------------------------------------------------------------------------------------

\section{Method}
\subsection{Logistic Regression}
As a start, we applied logistic regression on the data using Scikit-Learn's implementation\cite{scikit-learn}. The $L2$-penalized model minimizes the cost function below: $$\min_{w,c} \sum_{i=1}^n \log(e^{-y_i(X_i^Tw+c)}+1) + \frac{1}{2}w^Tw$$
The solver implemented in the code uses a coordinate descent algorithm, and the detailed proof can be found in Hsieh 2008 \cite{hsieh2008dual}
\subsection{Support Vector Machine}
Next, we used a linear support vector classifier from Scikit-Learn \cite{scikit-learn} to predict the tennis match outcome. Given the training vectors $x_i \in \mathbb{R}^n$ and the label vector $y = \{1,-1\}$ where 1 represents a win and -1 a loss, we can formulate the support vector machine as solving the following primal problem: $$min_{w,b, \varsigma} \sum_{i=1}^n \varsigma_i + \frac{1}{2}w^Tw$$ 
$$s.t. y_i(w^T\theta(x_i) + b) \geq 1 - \varsigma_i,    \varsigma_i \geq 0$$
Its dual is \begin{align*}
\min_\alpha \frac{1}{2}\alpha^TQ\alpha - e^T\alpha \\
s.t. y^T \alpha = 0,  0 \le \alpha \le 1 
\end{align*}
where $e$ is the vector of all ones, and $Q_{i,j} = y_i y_j K(x_i, x_j)$, and $K(x_i, x_j)$ is the kernel here. We tried different kernels including linear, RBF (where $K(x_i, x_j) = exp(-\gamma||x_i - x_j||^2)$), and polynomial (where $K(x_i, x_j) = (x_i^T x_j + c)^d$), and have documented performance below in Section 5.
\subsection{Other algorithms}
Moreover, we implemented \textbf{Naive Bayes} as a classifier, to check the model performance. We were curious to test if Naive Bayes classifier works well; and if so, whether this indicates that the features are \textit{conditionally independent} - i.e. break point percentage, rank difference, and all other features don't really relate to each other as long as I win the matches. Given a class variable $y = \{1,-1\}$  and a dependent feature vector $x_1$ through $x_n$, Bayes' theorem states the following relationship: $$P(y|x_1,...,x_n) = \frac{P(y)P(x_1,...,x_n|y)}{P(x_1,...,x_n)}$$ In particular, we have chosen the Gaussian Naive Bayes algorithm following the assumption that $X$ with respect to $y = \{1,-1\}$ is distributed according to a Gaussian distribution. This gives us the feature likelihood as $$P(x_i | y) = \frac{1}{\sqrt{2\pi \sigma_y^2}} exp(-\frac{(x_i - \mu_y)^2}{2\sigma_y^2})$$
And we use the following classification rule:
$$\hat{y} = arg \max_y P(y) \Pi_{i=1}^n P(x_i | y)$$
We mainly used Naive Bayes to benchmark model performance against logistic regression and SVC, as it is extremely fast to run.
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	Section Preliminary Experiments
%----------------------------------------------------------------------------------------

\section{Experiments, Results, Discussion}
We organized discussionin the following 4 questions. While a preliminary set of results were presented during the poster session, we had major overhauls in the past few days as we tried more models and fine-tuned parameters within existing algorithm choices. In particular, we found that SVC with a RBF kernel actually outperforms all our hybrid data models, albeit with a slightly bigger variance. As a result of this more thorough ad rigorous exploration, the results presented here will be different from the preliminary look from the poster session.  

\subsection{Parameters, Metrics}
For logistic regression, we chose L2 regularization with the \textit{liblinear} Algorithm \cite{hsieh2008dual} to use in the optimization solver. For small datasets, \textit{liblinear} is shown to perform really well. The penalty function makes sure that we don't overfit. 
For SVC, we explored multiple options such as linear, poly and RBF based on empirical performance on dev set, and used \textbf{RBF} (Gaussian) kernel for historical only data model and \textbf{linear} kernel for the rest. In general, we think this choice makes sense as RBF kernels are general purpose and usually the first thing to try when we are not processing text. We also used \textbf{squared hinge loss} ($\max(0,-)^2$) to penalize violated margins more strongly, based on emprical evidence on our particular dataset. Moreover, we used \textbf{L2 regularization} as well, as penalizing large weights tends to improve generalization (because it means that no input dimension can have a very large influence on the scores all by itself) and thus avoids overfitting. As evident by results presented below, this is part of our effort to encourage models to prefer smaller and more diffuse weight vectors (to counteract the fact that a very small subset of features account for most predictive power). The classifier is thus "encouraged" to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly, ideally. Lastly, we set the max iteration count to 100,000 as SVM may not converge. 

Cross-validation (10-fold) is done on all the results to ensure robustness. The general random split of train-dev-test (60:20:20) is also observed. We compared different models using the dev set for evaluation; once we had the best model, we passed that through the test set for a final model accuracy score. 

We mainly looked at \textbf{accuracy} as our success criterion, which calculates how many labels our model predicts correctly out of all the dev/test labels.  We used the term "\textbf{score}" interchangeably with accuracy in this report. 
\subsection{How well can we predict tennis matches?}
To evaluate model performance, it's important to note that for linear SVC and logistic regression, we capped the number of features to 15 - due to the penalty functions in place, we observed that accuracy drops sharply after we exceed 15 features using RFE. This can be clearly seen in Figure~\ref{fig:rfe}. 
\begin{figure}[h]
\caption{Accuracy vs. Number of Features}
  \label{fig:rfe}
  \centering
    \includegraphics[width=0.4\textwidth]{acc2}
\end{figure}
Our best model, which uses \textit{only} historical with a RBF-kernel SVC predicts with $94.6\%$ accuracy on test set, after being trained using cross-validation on the train set. The detailed performance comparison of each data method on \textit{test} score (with best performing machine learning algorithm, after being trained on the train set) can be seen in Table~\ref{tab:m1}. We have included the detailed dev set accuracy information in the appendix. 
\begin{table}[h]
\caption{Different Modeling Method Test Accuracy Comparison}
\label{tab:m1}
\begin{tabular}{ p{2.5cm} | p{2.5cm} | p{2cm}}
    \hline
    \textbf{Data Model} & \textbf{Best Performing Algorithm} & \textbf{Accuracy}  \\ \hline
   \textbf{Historical Data Only} &  SVC with RBF kernel & 86.5\%, std.dev = 4.2\%  \\ \hline
   \textbf{Real-Time Data Only, After Set 1} & Logistic Regression, capped at 15 features & 79.0\%, std.dev = 0\%  \\ \hline
   \textbf{Historical + Current Data (after Set 1)} &  Linear SVC, capped at 15 features & 80.1\%, std.dev = 0.7\%  \\ \hline
   \textbf{Historical + Current Data (after Set 2)} &  Linear SVC, capped at 15 features & 86.0\%, std.dev = 3.7\% \\ \hline
\end{tabular}
\end{table}
This is a surprising learning to us: using past performance alone really gives the best prediction. It also suggests that bringing in more data in the form of real-time stats actually worsens the model performance, due partly to the presence of penalty functions. However, once we bring in real-time stats after set 2, we almost have the on-par model performance again (with smaller variance). Thus, using real-time info seems to reduce variances, as models built on only historical data show a greater \textbf{variance} compared to any hybrid data models. 

Given the particular kernel choice of RBF, the results generated new hypotheses for us: could it mean that tennis match features, at least those we extracted, are not linearly separable but become separable on a higher dimension? Is using hinge loss strictly better than logistic loss? Our findings land support to these initial guesses that we can extend in our future work, to understand more about tennis match data. 
\subsection{Does using current, real-time info improve accuracy?}
No, it does not. Historical data alone, when coupled with SVC on RBF kernel, gives by far the best performance in all our runs. We have included confusion matrices below to to further compare performances (Tables \ref{tab:h1}, \ref{tab:h4}). 
\begin{table}[h]
\caption{Historical Only}
\label{tab:h1}
\begin{tabular}{ l | l | l}
    \hline
    & \textbf{TRUE} & \textbf{FALSE}  \\ \hline
   \textbf{POSITIVE} &  52.7\% & 0.1\%  \\ \hline
   \textbf{NEGATIVE} & 47.1\% & 0.1\%  \\ \hline
\end{tabular}
\end{table}
\begin{table}[h]
\caption{Historical + After Set 2}
\label{tab:h4}
\begin{tabular}{ c | c | c}
    \hline
    & \textbf{TRUE} & \textbf{FALSE}  \\ \hline
   \textbf{POSITIVE} &  42.9\% & 6.3\%  \\ \hline
   \textbf{NEGATIVE} &44.4\% & 6.4\%  \\ \hline
\end{tabular}
\end{table}

\subsection{What features are the most predictive, for linear models?}
Using recursive feature elimination (RFE) across our data models and linear algorithms, we find that \textbf{TTL} (total points won), which indicates how many points one player has won in the match up till our cut-off point, is the most important feature. Adding more features only leads to minimal improvement at best, for linear algorithms. In most cases, having \textbf{TTL} alone almost always yields the best performance, especially with penalty function enabled. Other top features include: \textbf{RCV} (\% of return points won) from real-time data, \textbf{BPP} (breaking point saving \%) if only historical match data are used. 
The graphs of how accuracy changes with number of features are included below in Figure~\ref{fig:rfe}; notice how it drops over time due to L2 regularization in both methods. 

\subsection{Diagnostics}
While fine-tuning our hybrid data models (in an effort to beat historical only data model), we observed the problem of \textbf{high bias} manifested as the small gap between training and test errors after plotting the learning curves for logistic regression model in Figure~\ref{fig:lg}, corroborated by learning curves from linear SVC in Figure~\ref{fig:lsvc}. 
\begin{figure}[h]
\caption{Logistic Regression Learning Curve}
  \label{fig:lg}
  \centering
    \includegraphics[width=0.4\textwidth]{lr_lc2}
\end{figure}

\begin{figure}[h]
\caption{Linear SVC Learning Curve}
  \label{fig:lsvc}
  \centering
    \includegraphics[width=0.4\textwidth]{lsvc_lc2}
\end{figure}

To combat high bias, we turn to unimplemented new features as this is the best solution outlined in Andrew Ng's error analysis slides. Despite other available features, our models learned that the information contained in other additional features (such as number of double faults or aces, etc.) is mostly already captured by \textbf{TTL}.  Therefore, we need to find features that are complementary to \textbf{TTL}, revealing information that could let to a "comeback" even if the player is at the moment behind, in order to get a more accurate prediction. Naturally, we think of a player's endurance and resillience, and look for features that could reflect similar traits. The two features added after the poster session for this purpose were: \begin{enumerate}
\item \textbf{duration win/loss rate}: Average duration of games won - average duration of games lost
\item \textbf{age diff win/loss rate}: Average age of opponent beaten - average age of opponent lost to
\end{enumerate}

However, we realized only the historical data model benefited slightly after adding these two features, using the original "best" algorithm. We do not observe any change in performance or top features' relative rankings when using other algorithms.
%----------------------------------------------------------------------------------------
%	Conclusion
%----------------------------------------------------------------------------------------
\section{Future Work}
For future work, there are still unimplemented features that we think will improve the model accuracy, such as: the (relative) importance of tournaments/matches for player, the number of matches a player had in the days before this match (as a proxy for fatigue), the tightness in schedule and intensity of those matches, number of matches the player played before in similar environment (location, weather, surface), time decaying factor, etc. It will also be interesting to see if historical + current data give better performance with updated feature lists.
\section{Conclusion}
The use of common opponent model and difference variables already offer high predictive power when coupled with SVC with a RBF kernel. And it's insightful to learn that for real-time prediction and linear models, total points won from before can really tell who is more likely to win, and a player's performance when faced with break points directly indicates his chance of winning. We also learnt that when making in-game predictions, historical performance data matter the most for accuracy and in-game stats hardly matter - i.e. most players actually carry some kind of "memory" when playing.  Moreover, we showed that high bias is the main issue for tennis prediction models, and any future model should take this into consideration by coming up with creative features derived from detailed point-by-point data for the maximal gain in accuracy. 
%----------------------------------------------------------------------------------------
%	Section Contributions
%----------------------------------------------------------------------------------------
\section{Appendix}
\subsection{Results: Dev set score for all algorithms}
\label{sec:app}
Values subject to random seed and fluctuations. 
\begin{itemize}
\item \textbf{Historical data only}: logistic 0.705, linear SVC 0.512, SVC + RBF kernel for 15 features 0.869, SVC + poly kernel for 15 features 0.455, SVC + linear kernel for 15 features 0.583, \textbf{SVC + RBF kernel for all features 0.891}, SVC + poly kernel for all features 0.476, SVC + linear kernel for all features 0.470, Naive Bayes with 15 features 0.676, Naive Bayes with all features 0.673
\item \textbf{Real-time data only}: \textbf{logistic 0.786}, linear SVC 0.606, SVC + RBF kernel for 15 features 0.642, SVC + poly kernel for 15 features 0.529, SVC + linear kernel for 15 features 0.663, SVC + RBF kernel for all features 0.642, SVC + poly kernel for all features 0.529, SVC + linear kernel for all features 0.663, Naive Bayes with 15 features 0.786, Naive Bayes with all features 0.786
\item \textbf{Historical + After 1st set}:  logistic 0.790,  linear SVC 0.805, SVC + RBF kernel for 15 features 0.776, SVC + poly kernel for 15 features 0.765, \textbf{SVC + linear kernel for 15 features 0.805}, SVC + RBF kernel for all features 0.505, SVC + poly kernel for all features 0.587, SVC + linear kernel for all features 0.554, Naive Bayes with 15 features 0.780, Naive Bayes with all features 0.772
\item \textbf{Historical + After 2nd set}: logistic 0.855, \textbf{linear SVC for 15 features 0.860},  SVC + RBF kernel for 15 features 0.789, SVC + poly kernel for 15 features 0.815, SVC + RBF kernel for all features 0.504, SVC + poly kernel for all features 0.454, SVC + linear kernel for all features 0.570, Naive Bayes with 15 features 0.831, Naive Bayes with all features 0.831
\end{itemize}
\section{Contributions}
\subsection{Yang "Eddie" Chen}
\begin{itemize}
\item Researched, reviewed and synthesized relevant literature for applicable ideas and the Related Work section, such as Clarke and Dyte \cite{Clarke2010} and O'Malley \cite{omalley} for difference variable, and Knottenbelt \cite{KNOTTENBELT20123820} for the common opponent model
\item Implemented the Common Opponent Model to look up players stats from common opponents
\item Generated ATP Match results for labels
\item Applied math fomulations from machine learning models \& techniques learned from class, namely logistic regression, SVM and Naive Bayes. 
\item Conducted error analyses along with Yi \& Yubo to understand the root cause of our performance curve, and proposing new checks and solutions. 
\item Typsetted (\LaTeX) the project milestone report, contributed in the poster making, and wrote, typesetted \& synthesized the final report. 
\end{itemize}

\subsection{Yubo Tian}
\begin{itemize}
\item Feature extraction: generated real-time set-level performance data from Sackmann's MCP \cite{tennis_charting}
\item Data labeling: extracted set level match results from player points information from Sackmann's MCP \cite{tennis_charting}
\item Data processing: joined match-general information, historical data and real-time match data; fixed various issues encountered during the process; wrote scripts to generate all data and labels upon one click
\item Initial modeling/error analysis: trained logistic regression model on joined data
\item Feature analysis and improvement: after initial modeling, analyzed feature selection results and train/dev errors, designed and extracted additional features in an attempt to fix the high bias problem exposed
\item Plot and Report: wrote scripts to plot learning curves; contributed to milestone/ final report contents.
\end{itemize}

\subsection{Yi Zhong}
\begin{itemize}
\item Feature extraction: get match details related features from MatchChartingProject, 
\item Data processing: Joined data by linking real-time match data from MCP \cite{tennis_charting} to match details data from ATPMatches \cite{tennis_atp} (methods: playerID, playerName etc) (with Yubo).
\item Modeling: Wrote the script for logistic, SVC and other model exploration for different data model
\item Error analysis: Wrote the graph script for recursive feature elimination, confusion matrix, feature ranking
\end{itemize}

\bibliographystyle{plain}
\bibliography{ref} 

\end{document}