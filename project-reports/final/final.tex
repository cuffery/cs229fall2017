%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=10pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=0.8in]{geometry} % this controls page margin
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
%\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
%\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

%\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{Stanford CS 229 Fall 2017} \\ [20pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\Large Final Project Report: \\
\Large Real Time Tennis Match Prediction Using Machine Learning\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Yang "Eddie" Chen, Yubo Tian, Yi Zhong} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}
\twocolumn[
\maketitle % Print the title
]
%----------------------------------------------------------------------------------------
%	Section 1 Motivation
%----------------------------------------------------------------------------------------
\begin{abstract}
Sports bring unpredictability and a lucrative betting industry trying to predict the unpredictable. While past work on predicting outcome for tennis matches focused on pre-game prediction using historical data, this project adopts an innovative data model by combining both historical performance data and real-time player in-game stats, and apply machine learning to predict tennis match outcome in-game - namely, after the 1st set and after the 2nd set. This project explores and compares four data models: (1) historical data only (pre-game prediction; baseline), (2) current after set 1 data only (in-game prediction), (3) historical + current after set 1 data (in-game prediction), and (4) historical + current after set 2 data (in-game prediction), with various machine learning techniques such as logistic regression, support vector classification (SVC) with linear, rbf and poly kernels, neural network, and Naive Bayes. Feature selection techniques such as recursive feature elimination, and principal component analysis were employed as well. W`e find that historical performance data do not increase accuracy when making in-game prediction with 1st set performance data, and in fact accuracy and precision both improved when in-game data are introduced. More importantly, a thorough analysis on results was done to reveal and understand the predominant challenge in this data model - high bias, as our collected feature set does not cover edge cases. \textbf{TTL} (total points won) is found to be the most dominant and predictive feature, with other standout features also revealed. From there, the project outlines the future work needed to combat the high bias issue by proposing other features that could be used but not included in the current project. 
\end{abstract}

\section{Introduction}
In the 2017 Stuttgart Open, Roger Federer, then an 18-time grand slam champion, lost to the world No. 302 player Tommy Haas on a grass court, which hadn't happened since 2002.  Based on past performance alone, almost any model would have predicted a Federer win in pre-game bets. This motivates us to apply machine learning to predict tennis matches in real time; in particular, we want to apply a hybrid model that combines historical and real-time, in-game information.
%----------------------------------------------------------------------------------------
%	Section - Method
%----------------------------------------------------------------------------------------

\section{Related Work}
Extensive research has been done on tennis as it is an ideal candidate for using hierarchical probability models: a match consists of a sequence of sets, which in turn consist of several games, which in turn consist of a sequence of points. Knottenbelt \cite{KNOTTENBELT20123820} proposed a common opponent model to extract features between two players that is found to be very predcitve in his stochastic models, while James \cite{omalley} has done thorough study on probability Formulas and statistical analysis on tennis matches. Recent researchers began to apply machine learning on tennis pre-game predictions based on past performance (including previous encounter, current ranking, etc), such as Sipko \cite{tennis1}, and Madurska \cite{tennis2setbyset} - whose by-set method inspired us as well. 

Our project seeks to apply classification algorithms from machine learning to model men's professional singles matches by using both pre-game, historical performance calcualted via a common opponent model \cite{KNOTTENBELT20123820}, and real-time, in-game stats \cite{tennis_charting} \cite{tennis2setbyset}. As pointed out by Sipko \cite{tennis1}, this in-game approach "allows the model to capture the change in a player's performance over the course of the match. For example, different players fatigue during a match in different ways." We hope that results from our predictions can be extended to give real-time coaching advice to support game strategy decision.
%----------------------------------------------------------------------------------------
%	Section - Method
%----------------------------------------------------------------------------------------

\section{Dataset and Features}
\subsection{Dataset \& pre-processing}
This project employs two open source datasets mainly for labels and features, by Jeff Sackmann \cite{tennis_atp} and \cite{tennis_charting}. The \textit{Match Charting Project} contains set level data from 1442 matches, spanning from 1974 to 2017. \textit{Tennis ATP} dataset contains match level data from 1969 to 2017.
\begin{itemize}
\item \textbf{Date}: Matches from 2000 - 2017 are used as samples (1415 matches)
\item \textbf{Remove duplicates via match id}: Since both datasets are manually entered via crowdsourcing, there are some inaccuracy and duplication that require our pre-processing.
\item \textbf{Remove Davis Cup}: Davis Cup is the World Cup of tennis, where players play for their country.  We decided to remove all Davis Cup entries since group matches may include strategic moves that are considerably different from individual tournaments.
\item \textbf{Merge Tennis ATP and Match Charting Project}: Real-time stats are obtained the \textit{Match Charting Project} dataset, whereas historical performance for both players is computed from \textit{Tennis ATP}. Two datasets are merged on player's first and last name, with players of the same names removed manually.
\item \textbf{Feature Standardization} We are unable to extract all features listed in Sipko \cite{tennis1}.  While extracting features from \textit{Tennis ATP} was easy, the \textit{Match Charting Project} data proves much more challenging.  We managed to extract a sbset of all possible features from processing the point-by-point data in the \textit{Match Charting Project}, and standardized between the historical and current performance.
\end{itemize}
\subsection{Feature Extraction}
\label{sec:label}
Original dataset we used denotes Player 1 as the winner and Player 2 as the loser. While processing, we decide to represent each set in each match with two rows, one from each player's perspective, and label the match result accordingly. Thus we have one row per player per set per match, containing features from historical data and real time data. Historical data includes match details and player's past performance; real-time data, for the scope of this project, will be defined as the performance from all previous sets in the match:
\begin{enumerate}
\item A vector of input features (\textbf{X}), describing the performance in previous set and historical performance from one player's perspective
\item A target value (\textbf{y}), indicating the match outcome. $y \in` \{1,-1\}$
\end{enumerate}
\subsubsection{Symmetric Feature Representation}
There are two approaches to represent features from two players.  First, we can have two features for the same metric (e.g. $RANK_1$ and $RANK_2$).  This approach doubles the number of features, but also provides a quantitative measure that we can compare across all players.  Second, we can look at \textit{differences} between two players (e.g. $RANK_{DIFF} = RANK_1 - RANK_2$).  This would reduce the number of features, but does not allow the model to compare across players.  Previous research has shown that the difference variables are predictive enough \cite{tennis1} \cite{omalley} for past performance (on match level).  For current performance (on set level), we evaluated the performance of both approaches. There was no difference in terms of accuracy, hence the difference model was used for current match as well for consistency.
\subsubsection{Common Opponent Model for Past Performance}
We use a method proposed by Knottenbelt \cite{KNOTTENBELT20123820} and extended by Sipko \cite{tennis1} to construct \textit{difference variables} capturing player characteristics and past performance, by using their \textbf{common opponents} to achieve a fair comparison. The detailed code can be found in our codebase, while the process works as follows: 
\begin{itemize}
\item Identify a set of common opponents between the two players of a given match.
\item Compute the average performance of both players against the common opponent.
\item Take the difference between average performance for each extracted feature.
\end{itemize}
\subsubsection{Edge Cases for Common Opponents}
We have run into 2 edge cases: (1) when at least one of the players is new; (2) when two players do not have any common opponents. Upon investigation, our solution is: (1) when the player is new with no historical data, we remove the samples, which consist of 84 data points out of 1421 matches; (2) when two players do not have any common opponents, we compute the average for each player (regardless of opponents) and take the difference, which consist of 74 out of 1421 matches. Note, as described in Section~\ref{sec:label}, our final sample size will contain $2x * 1421$ rows, where $x$ is the number of sets played in each match.
\subsection{Feature Lists}
\label{sec:feat}
\subsubsection{Features from historical match performance}
Features are the \textit{difference variables} averaged for both players obtained from a common opponent model \cite{KNOTTENBELT20123820}, unless otherwise noted. They are:
\begin{itemize}
\item \textbf{Performance Features}: Number of aces, double faults, break points faced and saved; Percentages of winning on first and second serves, and first serve in rate.
\item \textbf{Match Details}: Match duration, number of serve games, number of serve points per game
\item \textbf{Player Bio}: ATP Rank, ATP Rank Points, same handedness, height
\end{itemize}
\subsubsection{Features from current match performance}
Features are the difference in average for a snapshot in the match: after set 1, and after set 2. They are: 
\begin{itemize}
\item \textbf{Performance Features} Number of aces, double faults, winners and total points won; Percentage of winning on first and second serve, return points won, first serve in
\item \textbf{Match Details} Surface (hard, grass, or clay), match type
\item \textbf{Player Bio} Same handedness
\end{itemize}

%----------------------------------------------------------------------------------------
%	Section - Method
%----------------------------------------------------------------------------------------

\section{Method}
\subsection{Logistic Regression}
As a start, we built a logistic regression on the data using Scikit-Learn's implementation (add citation). The $L2$-penalized model minimizes the cost function below: $$\min_{w,c} \sum_{i=1}^n \log(e^{-y_i(X_i^Tw+c)}+1) + \frac{1}{2}w^Tw$$
The solver implemented in the code uses a coordinate descent algorithm, and the detailed proof can be found in Hsieh 2008 \cite{hsieh2008dual}
\subsection{Support Vector Machine}
Next, we used a linear support vector classifier from scikit-learn \cite{scikit-learn} to predict the tennis match outcome. Given training vectors $x_i \in \mathbb{R}^n$ and the label vector $y = \{1,-1\}$ where 1 represents a win and -1 represents a loss, we can formulate the support vector machine as solving the following primal problem: $$min_{w,b, \varsigma} \sum_{i=1}^n \varsigma_i + \frac{1}{2}w^Tw$$ 
$$s.t. y_i(w^T\theta(x_i) + b) \geq 1 - \varsigma_i,    \varsigma_i \geq 0$$
Its dual is \begin{align*}
\min_\alpha \frac{1}{2}\alpha^TQ\alpha - e^T\alpha \\
s.t. y^T \alpha = 0 \\
0 \le \alpha \le 1 
\end{align*}
where $e$ is the vector of all ones, and $Q_{i,j} = y_i y_j K(x_i, x_j)$, and $K(x_i, x_j)$ is the kernel here. We tried different kernels including linear, RBF, and poly which give similar performances. 
\subsection{Other algorithms}
Moreover, we've implemented \textbf{Naive Bayes} as a classifier, to check the model performance. We were curious to test if Naive Bayes classifier works well; and if so, if it indicates that the features are \textit{conditionally independent} - i.e. break point percentage, rank difference, and all other features don't really relate to each other as long as I win the matches. Given a class variable $y = \{1,-1\}$  and a dependent feature vector $x_1$ through $x_n$, Bayes’ theorem states the following relationship: $$P(y|x_1,...,x_n) = \frac{P(y)P(x_1,...,x_n|y)}{P(x_1,...,x_n)}$$
In particular, we have chosen the Gaussian Naive Bayes algorithm following the assumption that $X$ with respect to $y = \{1,-1\}$ is distributed according to a Gaussian distribution. This gives us the feature likelihood as $$P(x_i | y) = \frac{1}{\sqrt{2\pi \sigma_y^2}} exp(-\frac{(x_i - \mu_y)^2}{2\sigma_y^2})$$
And we use the following classification rule:
$$\hat{y} = arg \max_y P(y) \Pi_{i=1}^n P(x_i | y)$$
We mainly used Naive Bayes to benchmark model performance against logistic regression and linear SVC, as it is extremely fast to run.
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	Section Preliminary Experiments
%----------------------------------------------------------------------------------------

\section{Experiments, Results, Discussion}
We organized discussion around results and experiments in the following 4 questions. We mainly recorded results using logistic regression and linear SVC, for their excellent performances under our various data models. Moreover, cross-validation ($10-fold$) is done on all the results to ensure robustness. The general random split of train-dev-test (60:20:20) is also observed. Wr compared different models using the dev set after training models on the train set; once we had the best model, we passed that through the test set for final model evaluation. 
\subsection{Parameters}
TODO
\subsection{How well can we predict tennis matches?}
Our best model, which uses both historical and in-game performance with SVC predicts with $88\%$ accuracy on dev set when 2 sets have been played
\subsection{Does using current, real-time info improve accuracy?}
TODO
\subsection{What features are the most predictive?}
TODO
\subsection{Diagnostics}
TODO

In order to combat high bias, we turn to other, additional features as this is the best solution outlined in Andrew Ng's error analysis slides. As the result shown, once real time data of a tennis match is available, the prediction result is largely dominated by one feature: \textbf{TTL}, or total points won, which indicates how many points one player has won in the match up till our cut-off point (either after Set 1 or 2, depending on the run). Although many other features are available in our real time data, the model learned that the information contained in other additional features (such as number of double faults or aces, etc.) is generally captured by \textbf{TTL}.  Hence, in order to get a more accurate prediction, we need to find features that are complement to \textbf{TTL}, revealing information that could let to a "come back" even if the player is at the moment behind.
 
Two additional features that we added for this purpose were: \begin{enumerate}
\item \textbf{duration win/loss rate}: We first calculate the average duration (in minutes) of the past matches each player won or lost respectively, using the Common Opponent Model as described before, then calculate the difference of this historical data. When predicting the result of a new match, we divide the difference by the elapsed time of the match that we are trying to predict. 
\item \textbf{age diff win/loss rate}: For this feature, we calculate the average age difference of the past matches each player won or lost respectively, using the Common Opponent Model. Then, when predicting a new match, we divide the difference by the age difference between the two players in the match that we are trying to predict.
\end{enumerate}
\subsection{Historical, Match-level Dataset: Logistic Regression Model}
Applying a train-test split of 2:1, we trained a logistic regression model on $\frac{2}{3}$ of all the ATP matches from 2012. Following the labeling from Section~\ref{sec:label}, we set $label = 1$ for all the matches with Player 1 being the Winner (notation-only), and then duplicated the entire dataset but swapped Player 1 and Player 2 and set $label = 0$, as well as inverted all difference features.

We achieved an impressive accuracy rate of 87.8\% just using the historical, match-level dataset with common opponent model and difference variables. 

\subsection{Historical, Match-level Dataset: Naive Bayes Classifier}
We also ran the data through a Gaussian-based Naive Bayes classifier, which showed an impressive 86.9\% accuracy rate (misclassifying 718 points out of 5496). As expected, it was also extremely fast. The impressive performance indicates that the extracted features might indeed be conditionally independent of each other, which is something we never thought of before, and look to explore in the next steps. On a first glance, it seems to suggest that given Player $i$ won the match, his/her performance on aces and rank point difference or double faults are indeed almost independent. We were surprised by the results, and more sanity check is needed before we put ourselves behind this. 
\subsection{Real-time, Set-level Dataset: Logistic Regression Model}
Additional analysis were done on real-time, set-level data using features listed in Table~\ref{tab:features2}. We separate our data into two groups: (1) features calculated from performance after 1st set, (2) features calculated from performance after 2nd set.  Our hypothesis is that parameters trained on the second group has higher accuracy, since more information regarding the match is known.  Our model support this hypothesis.  Accuracy on test set for (1) is 77.8\%, for (2) is 83.9\%.  The difference in accuracy between training and testing set is less than 1\%, which suggests we do not have an under-fitting problem.
\subsubsection{Error Analysis}
Same error analysis was done on the logistic model.  We found that the most important features are: \textbf{W2SP} and \textbf{FSP}.
\subsection{Real-time, Set-level Dataset:  SVM}
In addition, we ran the data through a SVM model for the same comparison as logistic regression analysis.  We found that while train accuracy increased by around 1\%, test accuracy are about 10\%  lower for (1) and (2).  This suggest for SVM may be overfitting the dataset, we will need to do \textbf{feature selection} for SVM.

%----------------------------------------------------------------------------------------
%	Conclusion
%----------------------------------------------------------------------------------------
\section{Future Work}
There are multiple features that we think will improve the model accuracy, but did not include them in our training due to time constraints and limitations in data availability. Such features include: the (relative) importance of tournament/match for the player, number of matches player shad in the days before this match (as a proxy for fatigue), the tightness in schedule/intensity of those matches, number of matches the player played before in similar environment (location, weather, surface), etc.
\section{Conclusion}
The use of common opponent model and difference variables already offer high predictive power in simple machine learning models such as logistic regression and Naive Bayes, and it's insightful to learn that for real-time prediction, total points won from before can really tell who is more likely to win, and a player's performance when faced with break points directly indicates his chance of winning. We also learnt that when making in-game prediction with 1st set performance data, historical performance data do not increase accuracy - i.e. most players actually do get "fresh starts".  Moreover, when in-game data are introduced, accuracy and precision both improved. We also showed that high bias is the main issue for tennis prediction models, and any future model should take this into consideration. 
%----------------------------------------------------------------------------------------
%	Section Contributions
%----------------------------------------------------------------------------------------
\section{Appendix}
Real appendix goes here, takes page count

\section{Contributions}
\subsection{Yang "Eddie" Chen}
\begin{itemize}
\item Researched, reviewed and synthesized relevant literature for applicable ideas and the Related Work section, such as Clarke and Dyte \cite{Clarke2010} and O'Malley \cite{omalley} for difference variable, and Knottenbelt \cite{KNOTTENBELT20123820} for the common opponent model
\item Implemented the Common Opponent Model to look up players stats from common opponents
\item Generated ATP Match results for labels
\item Applied math fomulations from machine learning models \& techniques learned from class, namely logistic regression, SVM and Naive Bayes. 
\item Conducted error analyses along with Yi \& Yubo to understand the root cause of our performance curve, and proposing new checks and solutions. 
\item Typsetted (\LaTeX) the project milestone report, contributed mostly in the poster making, and typesetted \& synthesized the final report. 
\end{itemize}

\subsection{Yubo Tian}
\begin{itemize}
\item Feature extraction: generated real-time set-level performance data from Sackmann's MCP \cite{tennis_charting}
\item Data labeling: extracted set level match results from player points information from Sackmann's MCP \cite{tennis_charting}
\item Data processing: joined match-general information, historical data and real-time match data; fixed various issues encountered during the process; wrote scripts to generate all data and labels upon one click
\item Initial modeling/error analysis: trained logistic regression model on joined data
\item Feature analysis and improvement: after initial modeling, analyzed feature selection results and train/dev errors, designed and extracted additional features in an attempt to fix the high bias problem exposed
\item Plot and Report: wrote scripts to plot learning curves; contributed to milestone/ final report contents.
\end{itemize}

\subsection{Yi Zhong}
\begin{itemize}
\item Feature extraction: get match details related features from MatchChartingProject, 
\item Data processing: Joined data by linking real-time match data from MCP \cite{tennis_charting} to match details data from ATPMatches \cite{tennis_atp} (methods: playerID, playerName etc) (with Yubo).
\item Modeling: Wrote the script for logistic, SVC and other model exploration for different data model
\item Error analysis: Wrote the graph script for recursive feature elimination, confusion matrix, feature ranking
\end{itemize}

\bibliographystyle{plain}
\bibliography{ref} 

\end{document}